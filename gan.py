# -*- coding: utf-8 -*-
"""GAN_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15lulAVRNhZ85zSDQrBTzULXRCrSbQYo0
"""

import os
from google.colab import drive
drive.mount('/content/drive')
folder_path = '/content/drive/My drive/MNIST/GAN'

import tensorflow
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist
#from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint

(train_images, _), (test_images, _) = mnist.load_data()
#images = (np.concatenate([train_images, test_images]) - 127.5)/127.5
images = (train_images - 127.5)/127.5
images = images.reshape(-1, 28, 28, 1)

latent_dim = 128
#epochs = 200

# CNN Generator
def generator():
    inputs = layers.Input(shape = (latent_dim, ))
    layer = layers.Dense(512*4*4)(inputs)               # Changed from 256*4*4 to 1024*4*4
    layer = layers.Reshape((4, 4, 512))(layer)          # changed from (4, 4, 256) to (4, 4, 1024)
    layer = layers.Conv2DTranspose(filters = 256, kernel_size = 2, strides = (2, 2), padding = 'valid')(layer)  # upsize 8 by 8   # Changed filters from 256 to 512
    layer = layers.LeakyReLU(alpha = 0.2)(layer)
    layer = layers.Conv2DTranspose(filters = 256, kernel_size = 2, strides = (2, 2), padding = 'valid')(layer)  # Resize 16 by 16
    layer = layers.LeakyReLU(alpha = 0.2)(layer)
    layer = layers.Conv2D(filters = 256, kernel_size = 3, strides = (1, 1), padding = 'valid')(layer)           # Resize 14 by 14
    layer = layers.LeakyReLU(alpha = 0.2)(layer)
    layer = layers.Conv2DTranspose(filters = 256, kernel_size = 2, strides = (2, 2), padding = 'valid')(layer)  # Resize 28 by 28
    layer = layers.LeakyReLU(alpha = 0.2)(layer)

 #   layer = layers.Conv2D(filters = 512, kernel_size = 3, strides = (1, 1), padding = 'same')(layer)          # Extra layer added Changed it from 128 to 512
 #   layer = layers.LeakyReLU(alpha = 0.2)(layer)                                                              # Extra layer added
 #   layer = layers.Conv2D(filters = 512, kernel_size = 3, strides = (1, 1), padding = 'same')(layer)          # Extra layer added Changed it from 128 to 512
 #   layer = layers.LeakyReLU(alpha = 0.2)(layer)                                                              # Extra layer added


    outputs = layers.Conv2D(filters = 1, kernel_size = 3, strides = (1, 1), padding = 'same', activation = 'tanh')(layer)
    return Model(inputs, outputs)

# Generator
#def generator(input_shape = (latent_dim, )):
#    inputs = layers.Input(shape = input_shape)
#    layer = layers.Dense(64*7*7)(inputs)
#    layer = layers.Reshape((7, 7, 64))(layer)
#    layer = layers.Flatten()(layer)
#    token_embedding = layers.Reshape((-1, 64))(layer)
#    position_embedding = layers.Embedding(input_dim = 49, output_dim = 64, input_length = 49)(tensorflow.range(49))
#    embedding = token_embedding + position_embedding

#    for i in range(4):      # for model.h5, model_updated.h5 range(4)
#        attention_output = layers.MultiHeadAttention(8, 64)(embedding, embedding)
#        embedding = layers.Add()([embedding, attention_output])
#        embedding = layers.LayerNormalization(epsilon=0.001)(embedding)
#        ffn = layers.Dense(256, activation = 'relu')(embedding)
#        ffn = layers.Dense(64)(ffn)
#        embedding = layers.Add()([ffn, embedding])
#        embedding = layers.LayerNormalization(epsilon = 0.001)(embedding)

#    embedding = layers.Reshape((7, 7, 64))(embedding)
#    embedding = layers.UpSampling2D(size = (2,2), interpolation = 'bicubic')(embedding)
#    embedding = layers.LayerNormalization(epsilon = 0.001)(embedding)
#    embedding = layers.Reshape((14, 14, 64))(embedding)

#    embedding = layers.Flatten()(embedding)
#    embedding = layers.Reshape((-1, 64))(embedding)
#    p_embedding = layers.Embedding(input_dim = 196, output_dim = 64, input_length = 196)(tensorflow.range(196))
#    embedding = embedding + p_embedding

#    for i in range(4):      # for model.h5, model_updated.h5 range(4)
#        attention_output = layers.MultiHeadAttention(8, 64)(embedding, embedding)
#        embedding = layers.Add()([embedding, attention_output])
#        embedding = layers.LayerNormalization(epsilon=0.001)(embedding)
#        ffn = layers.Dense(256, activation = 'relu')(embedding)
#        ffn = layers.Dense(64)(ffn)
#        embedding = layers.Add()([ffn, embedding])
#        embedding = layers.LayerNormalization(epsilon = 0.001)(embedding)

#    embedding = layers.Reshape((14, 14, 64))(embedding)
#    embedding = layers.UpSampling2D(size = (2,2), interpolation = 'bicubic')(embedding)
#    embedding = layers.LayerNormalization(epsilon = 0.001)(embedding)
#    embedding = layers.Reshape((28, 28, 64))(embedding)
#    outputs = tensorflow.reduce_mean(embedding, axis = 3)
#    outputs = layers.Reshape((28, 28, 1))(outputs)
#    return Model(inputs, outputs)

# CNN Discriminator                        # Making discriminator a b bit stronger from two 128 to 256.
                                           # first layer to 256 from 64, second layer to 256 from 128
                                           # Getting sub optimal stability at 64, 256, 256, 128 ---> Increase to 512, 256, 256, 256, 128
def discriminator():
    inputs = layers.Input(shape = (28, 28, 1))
    layer = layers.Conv2D(filters = 512, kernel_size = 3, strides = (1, 1), padding = 'same')(inputs)                      # changed from 32 to 64
    layer = layers.LeakyReLU(alpha = 0.2)(layer)
    layer = layers.Conv2D(filters = 256, kernel_size = 2, strides = (2, 2), padding = 'valid')(layer)       # 14 by 14    # changed from 64 to 128
    layer = layers.LeakyReLU(alpha = 0.2)(layer)

 #   layer = layers.Conv2D(filters = 256, kernel_size = 2, strides = (1, 1), padding = 'same')(layer)       # Extra Layer Added
 #   layer = layers.LeakyReLU(alpha = 0.2)(layer)                                                           # Extra layer Added
 #   layer = layers.Conv2D(filters = 256, kernel_size = 2, strides = (1, 1), padding = 'same')(layer)       # Extra Layer Added
 #   layer = layers.LeakyReLU(alpha = 0.2)(layer)                                                           # Extra Layer Added

    layer = layers.Conv2D(filters = 256, kernel_size = 3, strides = (2, 2), padding = 'same')(layer)       # 8 by 8       # Changed from 128 to 256
    layer = layers.LeakyReLU(alpha = 0.2)(layer)
    layer = layers.Conv2D(filters = 256, kernel_size = 2, strides = (2, 2), padding = 'valid')(layer)      # 4 by 4
    layer = layers.LeakyReLU(alpha = 0.2)(layer)
  #  layer = layers.Conv2D(filters = 128, kernel_size = 2, strides = (1, 1), padding = 'same')(layer)      # 4 by 4           # Extra Layer Added
  #  layer = layers.LeakyReLU(alpha = 0.2)(layer)                                                                             # Extra Layer Added
    #layer = layers.Flatten()(layer)
    layer = layers.GlobalAveragePooling2D()(layer)    # changed it from Flatten
    outputs = layers.Dense(1, activation = 'sigmoid')(layer)
    return Model(inputs, outputs)

# Discriminator
#def discriminator(input_shape = (28, 28, 1)):
#    inputs = layers.Input(shape = input_shape)
#    token_embedding = layers.Conv2D(filters = 64, kernel_size = 4, strides = (4, 4))(inputs)
#    token_embedding = layers.Reshape((-1, 64))(token_embedding)

#    position_embedding = layers.Embedding(input_dim = 49, output_dim = 64, input_length = 49)(tensorflow.range(49))
#    embedding = token_embedding + position_embedding

#    for i in range(4):      # for model.h5, model_updated.h5 range(4)
#        attention_output = layers.MultiHeadAttention(8, 64)(embedding, embedding)
#        embedding = layers.Add()([embedding, attention_output])
#        embedding = layers.LayerNormalization(epsilon=0.001)(embedding)
#        ffn = layers.Dense(256, activation = 'relu')(embedding)
#        ffn = layers.Dense(64)(ffn)
#        embedding = layers.Add()([ffn, embedding])
#        embedding = layers.LayerNormalization(epsilon = 0.001)(embedding)

#    embedding = layers.GlobalAveragePooling1D()(embedding)
#    output = layers.Dense(1, activation = 'sigmoid')(embedding)
#    return Model(inputs, output)

g_model = generator()
d_model = discriminator()
d_model.compile(optimizer = Adam(learning_rate = 0.0002, beta_1 = 0.5), loss = 'binary_crossentropy', metrics = ['accuracy'])
g_model.summary()

# GAN
def GAN(g_model, d_model):
    d_model.trainable = False
    inputs = layers.Input((latent_dim, ))
    generated_images = g_model(inputs)
    outputs = d_model(generated_images)
    return Model(inputs, outputs)

GAN = GAN(g_model, d_model)
GAN.compile(optimizer = Adam(learning_rate = 0.0003, beta_1 = 0.5), loss = 'binary_crossentropy', metrics = ['accuracy'])

# Build a log file
with open('/content/drive/My Drive/MNIST/GAN/training_log.csv', 'w') as log_file:
    log_file.write('Epoch, Discriminator Loss, Discriminator Accuracy, Generator Loss, Generator Accuracy\n')

# Build model
def build_model(g, d, gan, dataset, noise_dim, epochs, batch_size):
    batch_per_epoch = int(dataset.shape[0]/batch_size)
    half_batch_size = int(batch_size/2)
    for i in range(epochs):
        d_losses = []
        d_accs = []
        g_losses = []
        g_accs = []
        for j in range(batch_per_epoch):
            indeces = np.random.randint(0, dataset.shape[0], half_batch_size)
            X_R = dataset[indeces]
            Y_R = np.ones((half_batch_size, 1))
            random_indeces = np.random.normal(0, 1, noise_dim*half_batch_size)
            random_inputs = random_indeces.reshape(half_batch_size, noise_dim)
            X_F = g.predict(random_inputs)
            Y_F = np.zeros((half_batch_size, 1))
            X, Y = np.vstack((X_R, X_F)), np.vstack((Y_R, Y_F))
            d_loss = d.train_on_batch(X, Y)
            X_gan = np.random.normal(0, 1, noise_dim*batch_size).reshape(batch_size, noise_dim)
            Y_gan = np.ones((batch_size, 1))
            g_loss = GAN.train_on_batch(X_gan, Y_gan)
            d_losses.append(d_loss[0])
            g_losses.append(g_loss[0])
            d_accs.append(d_loss[1])
            g_accs.append(g_loss[1])
            print('Epoch = %d, Step = %d/%d, d_loss = %.3f, gan_loss = %.3f' % (i+1, j+1, batch_per_epoch, d_loss[0], g_loss[0])) # i+1 as recursion starts from 0
        d_loss_avg = np.mean(d_losses)
        g_loss_avg = np.mean(g_losses)
        d_accs_avg = np.mean(d_accs)
        g_accs_avg = np.mean(g_accs)
        save(i, g, d, gan, dataset, latent_dim)
        with open('/content/drive/My Drive/MNIST/GAN/training_log.csv', 'a') as log_file:
             log_file.write(f'{i+1}, {d_loss_avg}, {d_accs_avg*100}, {g_loss_avg}, {g_accs_avg*100}\n')
        #if (i+1)%10 == 0:

# Save plots and model
def save(i, g, d, gan, dataset, latent_dim, n_samples = 200):
    X_R, Y_R = dataset[np.random.randint(0, dataset.shape[0], n_samples)], np.ones((n_samples, 1))
    d_r = d.evaluate(X_R, Y_R, verbose=0)
    X_F, Y_F = g.predict(np.random.normal(0, 1, latent_dim*n_samples).reshape(n_samples, latent_dim)), np.zeros((n_samples, 1))
    d_f = d.evaluate(X_F, Y_F, verbose=0)
    print('-- Accuracy real:%.0f%%, fake: %.0f%%' % (d_r[1]*100, d_f[1]*100))
    model_directory_R = '/content/drive/My Drive/MNIST/GAN/Save Model/Real_Images'
    model_directory_F = '/content/drive/My Drive/MNIST/GAN/Save Model/Fake_Images'
    os.makedirs(model_directory_R, exist_ok = True)
    os.makedirs(model_directory_F, exist_ok = True)
    save_plot_with_probs(X_R, d, os.path.join(model_directory_R,'real_images_%03d.png' % (i + 1)))
    save_plot_with_probs(X_F, d, os.path.join(model_directory_F, 'fake_images_%03d.png' % (i + 1)))

    save_plot(X_F, i)
    model_dir = '/content/drive/My Drive/MNIST/GAN/Save Model/Generator_Model/Model Weights'
    model_dir1 = '/content/drive/My Drive/MNIST/GAN/Save Model/Generator_Model/Generator Weights'
    model_dir2 = '/content/drive/My Drive/MNIST/GAN/Save Model/Generator_Model/Discriminator Weights'
    model_dir3 = '/content/drive/My Drive/MNIST/GAN/Save Model/Generator_Model/GAN Weights'
    os.makedirs(model_dir, exist_ok = True)
    os.makedirs(model_dir1, exist_ok = True)
    os.makedirs(model_dir2, exist_ok = True)
    os.makedirs(model_dir3, exist_ok = True)
    #filename = os.path.join(model_dir, 'generator_model_%03d.keras' % (i + 1))
    filename1 = os.path.join(model_dir1, 'generator_weights_%03d.keras' % (i + 1))
    filename2 = os.path.join(model_dir2, 'discriminator_weights_%03d.keras' % (i + 1))
    filename3 = os.path.join(model_dir3, 'gan_weights_%03d.keras' % (i + 1))
    #g.save(filename)
    g.save_weights(filename1)
    d.save_weights(filename2)
    gan.save_weights(filename3)

#model_dir4 = '/content/drive/My Drive/MNIST/GAN/Save Model/Generator_Model/Images with probability'
#os.makedirs(model_dir4, exist_ok = True)
model_dir5 = '/content/drive/My Drive/MNIST/GAN/Save Model/Generated Images'
os.makedirs(model_dir5, exist_ok = True)


def save_plot_with_probs(examples, model, filename, n=7):
    examples = (examples + 1) / 2.0

    for i in range(n * n):
        plt.subplot(n, n, 1 + i)
        plt.axis('off')
        plt.imshow(examples[i], cmap = 'gray')
        probability = model.predict(np.expand_dims(examples[i], axis=0))[0][0]
        plt.text(2, 2, f"{probability:.4f}", color='white', fontsize=8, bbox=dict(facecolor='black', alpha=0.7))
    #filename1 = os.path.join(model_dir4, filename)
    plt.savefig(filename)
    plt.close()

def save_plot(examples, epoch, n=7):
    examples = (examples + 1) / 2
    for i in range(n * n):
        plt.subplot(n, n, 1 + i)
        plt.axis('off')
        plt.imshow(examples[i], cmap = 'gray')
        filename = os.path.join(model_dir5, 'generated_plot%3d.png' % (epoch + 1))
        plt.savefig(filename)
        plt.close

model_dir = '/content/drive/My Drive/MNIST/GAN'
filename1 = os.path.join(model_dir, 'generator_weights_040.keras')
filename2 = os.path.join(model_dir, 'discriminator_weights_040.keras')
filename3 = os.path.join(model_dir, 'gan_weights_040.keras')

g_model.load_weights(filename1)
d_model.load_weights(filename2)
GAN.load_weights(filename3)

build_model(g_model, d_model, GAN, images, latent_dim, 100, 256)

import os
import random
import matplotlib.pyplot as plt
import matplotlib.animation as animation

def animate_frames(image_folder, file_prefix, start_index, end_index, num_frames, output_file):
    # Generate a list of file paths
    file_paths = [os.path.join(image_folder, f"{file_prefix}{i}.png") for i in range(start_index, end_index)]

    # Shuffle the list to randomize the frames
    random.shuffle(file_paths)

    # Select a subset of frames
    random_files = random.sample(file_paths, num_frames)

    # Create the figure and axis objects
    fig, ax = plt.subplots()

    # Initialize the image object
    img = ax.imshow(plt.imread(random_files[0]), cmap='gray')

    def update(frame):
        img.set_array(plt.imread(random_files[frame]))
        return img,

    ani = animation.FuncAnimation(fig, update, frames=num_frames, interval=200)

    # Save the animation to a file
    ani.save(output_file)

# Example usage
image_folder = '/content/drive/My Drive/MNIST/GAN/Save Model/Generated Images'
file_prefix = 'generated_plot '
start_index = 70
end_index = 100
num_frames = 15
output_file = '/content/drive/My Drive/MNIST/GAN/animated_frames.gif'

animate_frames(image_folder, file_prefix, start_index, end_index, num_frames, output_file)

g_model.load_weights('/content/drive/My Drive/MNIST/GAN/Save Model/Generator_Model/Generator Weights/generator_weights_096.keras')

noise = (np.random.normal(0, 1, (49, latent_dim)) + 1)/2.0
list = g_model.predict(noise)
for i in range(49):
    plt.subplot(7, 7, 1+i)
    plt.axis('off')
    plt.imshow(list[i, :, :, 0], cmap = 'gray')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Function to generate images from the generator
def generate_images(generator, num_samples):
    noise_dim = generator.input_shape[1]
    random_noise = np.random.normal(0, 1, (num_samples, noise_dim))
    return generator.predict(random_noise)

# Generate images for animation frames
num_frames = 15
num_samples_per_frame = 7 * 7  # 7x7 grid of images
generated_images_per_frame = []
for _ in range(num_frames):
    generated_images = generate_images(g_model, num_samples_per_frame)
    generated_images_per_frame.append(generated_images)

# Function to update the animation frames
def update(frame):
    plt.clf()
    plt.axis('off')
    images = generated_images_per_frame[frame]
    for i, img in enumerate(images):
        plt.subplot(7, 7, i+1)
        plt.imshow(img[:, :, 0], cmap='gray')
        plt.axis('off')

# Create animation
fig = plt.figure(figsize=(7, 7))
animation = FuncAnimation(fig, update, frames=num_frames, interval=200)

# Save animation as a gif
animation.save('/content/drive/My Drive/MNIST/GAN/generated_animation.gif', writer='imagemagick')
plt.show()